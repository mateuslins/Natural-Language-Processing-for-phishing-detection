{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASED ON THE WORK OF Sahingoz, Ozgur K.; Buber, Ebubekir; Demir, Onder; Diri, Banu\n",
    "#ARTICLE: Machine Learning Based Phishing Detection from URLs\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNÇÃO PARA SEPARAR POR MÚLTIPLOS CARACTERES\n",
    "\n",
    "def tsplit(string, delimiters):\n",
    "    \"\"\"Behaves str.split but supports multiple delimiters.\"\"\"\n",
    "    \n",
    "    delimiters = tuple(delimiters)\n",
    "    stack = [string,]\n",
    "    \n",
    "    for delimiter in delimiters:\n",
    "        for i, substring in enumerate(stack):\n",
    "            substack = substring.split(delimiter)\n",
    "            stack.pop(i)\n",
    "            for j, _substring in enumerate(substack):\n",
    "                stack.insert(i+j, _substring)\n",
    "            \n",
    "    return stack\n",
    "\n",
    "caracteres = [\"[\", \" \", \":\", \"`\", \"\\\\\", \"-\", \"=\", \"~\", \"!\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"(\", \")\", \"+\", \"\\\\\\\\\", \"[\", \"]\", \"{\", \"}\", \";\", \"'\", \"\\\\:\", \"|\", \"<\", \",\", \".\", \"/\", \"<\", \">\", \"?\", \"]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crjewelers', 'net', 'crjguest']\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.crjewelers.net/crjguest/\"\n",
    "\n",
    "lista = tsplit(url, caracteres)\n",
    "\n",
    "#FUNÇÃO DE REMOVER CARACTERES ADICIONAIS NA URL\n",
    "def remover_adicionais(i, lista):\n",
    "    if i == '':\n",
    "        lista.remove('')\n",
    "    elif i == \"http\":\n",
    "        lista.remove(\"http\")\n",
    "    elif i == \"www\":\n",
    "        lista.remove(\"www\")\n",
    "    elif i == \"\\n\":\n",
    "        lista.remove(\"\\n\")\n",
    "\n",
    "for i in lista[:]:\n",
    "    remover_adicionais(i, lista)\n",
    "\n",
    "print(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.cosmeticasaludable.com/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PEGANDO O TXT DE URLS_INVALIDOS\n",
    "\n",
    "f = open(\"urls_invalidos.txt\", encoding=\"utf8\")\n",
    "\n",
    "txt = f.readlines()\n",
    "\n",
    "print(txt[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['crjewelers', 'net', 'crjguest'], ['b2b', 'marketplace', 'opensourcescripts', 'com'], ['changtsi', 'com', 'js', 'newlogin', 'php\\n'], ['asterasepe', 'gr'], ['cosmeticasaludable', 'com']]\n",
      "5\n",
      "4\n",
      "['b2b', 'marketplace', 'opensourcescripts', 'com']\n"
     ]
    }
   ],
   "source": [
    "#TESTANDO O SPLIT NOS URLS INVALIDOS DO TXT\n",
    "\n",
    "invalidos = []\n",
    "\n",
    "for i in txt[:5]:\n",
    "    invalidosAux = tsplit(i, caracteres)\n",
    "    for j in invalidosAux[:]:\n",
    "        remover_adicionais(j, invalidosAux)\n",
    "    invalidos.append(invalidosAux)\n",
    "    \n",
    "print(invalidos)\n",
    "print(len(invalidos))\n",
    "print(len(invalidos[1]))\n",
    "print(invalidos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['changtsi', 'com', 'weuyrwbr']\n",
      "com\n"
     ]
    }
   ],
   "source": [
    "#TESTANDO COM A URL EXEMPLO\n",
    "\n",
    "example = \"www.changtsi.com/weuyrwbr/\"\n",
    "listaExample = tsplit(example, caracteres)\n",
    "\n",
    "for i in listaExample:\n",
    "    remover_adicionais(i, listaExample)\n",
    "\n",
    "print(listaExample)\n",
    "print(listaExample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acharIgual(palavra, invalidos):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < len(invalidos):\n",
    "        while j < len(invalidos[i]):\n",
    "            if palavra == invalidos[i][j]:\n",
    "                return palavra\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return \"não tem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "não tem\n"
     ]
    }
   ],
   "source": [
    "#TESTANDO PARA VER SE ACHA ALGUMA PALAVRA JÁ SEPARADA DA URL EXEMPLO QUE SEJA IGUAL A ALGUMA DO TXT DE INVÁLIDOS\n",
    "\n",
    "aux = acharIgual(listaExample[0], invalidos)\n",
    "print(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adidas', 'com', 'activision', 'blizzard', 'lorota1', 'crjguest', 'banking', 'wfbuwnuernfuefberyu']\n"
     ]
    }
   ],
   "source": [
    "#TESTANDO COM A URL EXEMPLO FINAL PROS OUTROS TESTES\n",
    "\n",
    "exemplo = \"www.adidas.com/activision-blizzard/lorota1/crjguest/banking/wfbuwnuernfuefberyu\"\n",
    "\n",
    "exLista = tsplit(exemplo, caracteres)\n",
    "for i in exLista:\n",
    "    remover_adicionais(i, exLista)\n",
    "\n",
    "print(exLista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adidas\n"
     ]
    }
   ],
   "source": [
    "#ABRINDO O TXT DAS MARCAS\n",
    "\n",
    "b = open(\"allbrands.txt\", \"r\", encoding=\"utf8\")\n",
    "\n",
    "allbrands = b.read()\n",
    "allbrandsList = allbrands.split(\"\\n\")\n",
    "\n",
    "print(allbrandsList[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "access\n"
     ]
    }
   ],
   "source": [
    "#ABRINDO O TXT DAS KEYWORDS\n",
    "\n",
    "c = open(\"keywords.txt\", \"r\", encoding=\"utf8\")\n",
    "\n",
    "keywords = c.read()\n",
    "keywordsList = keywords.split(\"\\n\")\n",
    "\n",
    "print(keywordsList[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO MOMENTO ESTÁ SÓ REMOVENDO DA LISTA, MAS NA PIPELINE INFORMA QUE AS PALAVRAS QUE FOREM IGUAIS A ALGUMA DOS ARQUIVOS\n",
    "#VAI PARA A FEATURE EXTRACTION\n",
    "\n",
    "def achar_igual(palavra, lista):\n",
    "    for i in lista:\n",
    "        if i == palavra:\n",
    "            return palavra\n",
    "    return \"não tem\"\n",
    "\n",
    "def achar_existentes(exLista, documento):\n",
    "    for i in exLista:\n",
    "        resposta = achar_igual(i, documento)\n",
    "        if resposta != \"não tem\":\n",
    "            exLista.remove(resposta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['activision', 'blizzard', 'lorota1', 'crjguest', 'wfbuwnuernfuefberyu']\n"
     ]
    }
   ],
   "source": [
    "#MÓDULO \"BRAND NAMES AND KEYWORDS CHECK\"\n",
    "\n",
    "achar_existentes(exLista, allbrandsList)\n",
    "achar_existentes(exLista, keywordsList)\n",
    "\n",
    "print(exLista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALGORITMO PARA DETECTAR PALAVRAS ALEATORIAS\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "accepted_chars = 'abcdefghijklmnopqrstuvwxyz '\n",
    "\n",
    "pos = dict([(char, idx) for idx, char in enumerate(accepted_chars)])\n",
    "\n",
    "def normalize(line):\n",
    "    \"\"\" Return only the subset of chars from accepted_chars.\n",
    "    This helps keep the  model relatively small by ignoring punctuation, \n",
    "    infrequenty symbols, etc. \"\"\"\n",
    "    return [c.lower() for c in line if c.lower() in accepted_chars]\n",
    "\n",
    "def ngram(n, l):\n",
    "    \"\"\" Return all n grams from l after normalizing \"\"\"\n",
    "    filtered = normalize(l)\n",
    "    for start in range(0, len(filtered) - n + 1):\n",
    "        yield ''.join(filtered[start:start + n])\n",
    "\n",
    "def train():\n",
    "    \"\"\" Write a simple model as a pickle file \"\"\"\n",
    "    k = len(accepted_chars)\n",
    "    # Assume we have seen 10 of each character pair.  This acts as a kind of\n",
    "    # prior or smoothing factor.  This way, if we see a character transition\n",
    "    # live that we've never observed in the past, we won't assume the entire\n",
    "    # string has 0 probability.\n",
    "    counts = [[10 for i in range(k)] for i in range(k)]\n",
    "\n",
    "    # Count transitions from big text file, taken \n",
    "    # from http://norvig.com/spell-correct.html\n",
    "    for line in open('big.txt'):\n",
    "        for a, b in ngram(2, line):\n",
    "            counts[pos[a]][pos[b]] += 1\n",
    "\n",
    "    # Normalize the counts so that they become log probabilities.  \n",
    "    # We use log probabilities rather than straight probabilities to avoid\n",
    "    # numeric underflow issues with long texts.\n",
    "    # This contains a justification:\n",
    "    # http://squarecog.wordpress.com/2009/01/10/dealing-with-underflow-in-joint-probability-calculations/\n",
    "    for i, row in enumerate(counts):\n",
    "        s = float(sum(row))\n",
    "        for j in range(len(row)):\n",
    "            row[j] = math.log(row[j] / s)\n",
    "\n",
    "    # Find the probability of generating a few arbitrarily choosen good and\n",
    "    # bad phrases.\n",
    "    good_probs = [avg_transition_prob(l, counts) for l in open('good.txt')]\n",
    "    bad_probs = [avg_transition_prob(l, counts) for l in open('bad.txt')]\n",
    "\n",
    "    # Assert that we actually are capable of detecting the junk.\n",
    "    assert min(good_probs) > max(bad_probs)\n",
    "\n",
    "    # And pick a threshold halfway between the worst good and best bad inputs.\n",
    "    thresh = (min(good_probs) + max(bad_probs)) / 2\n",
    "    pickle.dump({'mat': counts, 'thresh': thresh}, open('gib_model.pki', 'wb'))\n",
    "\n",
    "def avg_transition_prob(l, log_prob_mat):\n",
    "    \"\"\" Return the average transition prob from l through log_prob_mat. \"\"\"\n",
    "    log_prob = 0.0\n",
    "    transition_ct = 0\n",
    "    for a, b in ngram(2, l):\n",
    "        log_prob += log_prob_mat[pos[a]][pos[b]]\n",
    "        transition_ct += 1\n",
    "    # The exponentiation translates from log probs to probs.\n",
    "    return math.exp(log_prob / (transition_ct or 1))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['activision', 'blizzard', 'lorota1']\n"
     ]
    }
   ],
   "source": [
    "#RANDOM WORD DETECTION\n",
    "#TAMBÉM ESTÁ SÓ REMOVENDO, MAS DEVE IR PARA FEATURE SELECTION\n",
    "\n",
    "model_data = pickle.load(open('gib_model.pki', 'rb'))\n",
    "\n",
    "for i in exLista[:]:\n",
    "    l = i\n",
    "    model_mat = model_data['mat']\n",
    "    threshold = model_data['thresh']\n",
    "    naoAleatorio = avg_transition_prob(l, model_mat) > threshold\n",
    "    if naoAleatorio == False:\n",
    "        exLista.remove(i)\n",
    "        \n",
    "print(exLista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en', 'en_AU', 'en_CA', 'en_GB', 'en_US']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORTANDO A BIBLIOTECA DE DICIONARIO (SO DISPONIVEL PARA LINUX)\n",
    "\n",
    "import enchant\n",
    "\n",
    "disc = enchant.Dict(\"en_US\")\n",
    "enchant.list_languages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['activision', 'blizzard', 'lorota1']\n"
     ]
    }
   ],
   "source": [
    "#Remoção de dígitos\n",
    "from string import digits \n",
    "  \n",
    "def remove(list): \n",
    "    list = [''.join(x for x in i if x.isalpha()) for i in list] \n",
    "              \n",
    "    return list\n",
    "\n",
    "remove(exLista)\n",
    "    \n",
    "print(exLista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['activision', 'lorota1']\n"
     ]
    }
   ],
   "source": [
    "#checar existente no dicionario e menores que 3\n",
    "\n",
    "for i in exLista[:]:\n",
    "    if (disc.check(i) == True or len(i) <= 3):\n",
    "        exLista.remove(i)\n",
    "        \n",
    "print(exLista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ac', 'ct', 'ti', 'iv', 'vi', 'is', 'on', 'act', 'ion', 'vision'], ['lo', 'or', 'ta', 'rot', 'rota']]\n",
      "proceed :>\n",
      "proceed :>\n",
      "proceed :>\n",
      "proceed :>\n",
      "proceed :>\n",
      "proceed :>\n",
      "proceed :>\n",
      "proceed :>\n",
      "proceed :>\n",
      "proceed :>\n",
      "proceed :>\n",
      "proceed :>\n",
      "proceed :>\n",
      "proceed :>\n",
      "proceed :>\n"
     ]
    }
   ],
   "source": [
    "#dividir em substrings e comparar\n",
    "#não leva em consideração só as palavras com tamanho maior que 7, como no artigo original\n",
    "\n",
    "from nltk import everygrams\n",
    "\n",
    "substrings = []\n",
    "\n",
    "for i in exLista[:]:\n",
    "    word = i\n",
    "    subs = [''.join(_ngram) for _ngram in everygrams(word) if disc.check(''.join(_ngram)) and len(_ngram) > 1]\n",
    "    substrings.append(subs)\n",
    "\n",
    "print(substrings)\n",
    "\n",
    "for i in substrings:\n",
    "    for j in i:\n",
    "        if disc.check(j) == True:\n",
    "            print(\"proceed :>\")\n",
    "        else:\n",
    "            print(\"doesn't proceed :/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ac', 'ct', 'ti', 'iv', 'vi', 'is', 'on', 'act', 'ion', 'vision'], ['lo', 'or', 'ta', 'rot', 'rota']]\n"
     ]
    }
   ],
   "source": [
    "#COMPARANDO SUBSTRINGS COM PALAVRAS NOS ARQUIVOS\n",
    "\n",
    "for i in substrings:\n",
    "    achar_existentes(i, allbrandsList)\n",
    "    achar_existentes(i, keywordsList)\n",
    "    \n",
    "print(substrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALGORITMO LEVENSHTEIN DISTANCE\n",
    "\n",
    "def levenshtein(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    #print (matrix)\n",
    "    return (matrix[size_x - 1, size_y - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ac', 'ct', 'ti', 'iv', 'vi', 'is', 'on', 'act', 'ion', 'vision'], ['or', 'rot', 'rota']]\n"
     ]
    }
   ],
   "source": [
    "#SEPARA OS VALORES MENORES DE 2\n",
    "#ESTÁ REMOVENDO OS ACUSADOS PELO LEVENSHTEIN DISTANCE, MAS DEVEM IR PARA FEATURE EXTRACTION\n",
    "\n",
    "def calcular_distancia(palavra, lista):\n",
    "    for i in lista:\n",
    "        distan = levenshtein(palavra, i)\n",
    "        if distan < 2:\n",
    "            return \"procede\"\n",
    "    return \"nao procede\"\n",
    "\n",
    "for i in substrings:\n",
    "    for j in i:\n",
    "        resp1 = calcular_distancia(j, allbrandsList)\n",
    "        resp2 = calcular_distancia(j, keywordsList)\n",
    "        \n",
    "        if (resp1 == \"procede\" or resp2 == \"procede\"):\n",
    "            i.remove(j)\n",
    "        \n",
    "print(substrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
